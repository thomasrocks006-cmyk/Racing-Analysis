# Racing Analysis System - Master Implementation Plan

**Created**: November 8, 2025
**Status**: Phase 0 - Foundation
**Goal**: Build a high-accuracy horse racing prediction system with quantitative modeling + qualitative research fusion

---

## ğŸ¯ Project Goals

1. **Primary**: Produce calibrated win/place probabilities and fair odds per race
2. **Secondary**: Identify value opportunities vs market prices (Betfair/TAB)
3. **Tertiary**: Build reproducible backtesting framework with uncertainty quantification

**Non-Negotiables**:

- âœ… Strict event-time ordering (no data leakage)
- âœ… Reproducible runs with versioning
- âœ… Uncertainty quantification on all predictions
- âœ… Non-commercial personal use (scraping permitted)

---

## ğŸ—ï¸ System Architecture

### **Master Taxonomy â†’ Pipelines â†’ Fusion Model**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         MASTER TAXONOMY (Foundation)                          â”‚
â”‚  21 Categories + 3 Integration Matrices + Implementation Roadmaps            â”‚
â”‚  â€¢ Categories 1-17: Qualitative (Race context, form, intelligence)           â”‚
â”‚  â€¢ Categories 18-21: Quantitative (Speed, class, sectionals, pedigree)       â”‚
â”‚  â€¢ Integration Matrices: Cross-category synergies & conflict resolution      â”‚
â”‚  Reference: TAXONOMY_OVERVIEW.md                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚                            â”‚
                 â–¼                            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   DATA COLLECTION LAYER      â”‚  â”‚   DATA COLLECTION LAYER                  â”‚
â”‚   (Qualitative Sources)      â”‚  â”‚   (Quantitative Sources)                 â”‚
â”‚                              â”‚  â”‚                                          â”‚
â”‚ â€¢ Racing.com (official form) â”‚  â”‚ â€¢ Timeform ratings                       â”‚
â”‚ â€¢ Stewards reports           â”‚  â”‚ â€¢ Racing Victoria ratings                â”‚
â”‚ â€¢ Barrier trials             â”‚  â”‚ â€¢ Sectional times (ChampionData)         â”‚
â”‚ â€¢ Gear changes               â”‚  â”‚ â€¢ Pedigree databases                     â”‚
â”‚ â€¢ Market odds (Betfair/TAB)  â”‚  â”‚ â€¢ Historical results database            â”‚
â”‚ â€¢ Weather data               â”‚  â”‚ â€¢ Class/BenchMark records                â”‚
â”‚ â€¢ Expert commentary          â”‚  â”‚ â€¢ Speed maps & par times                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚                                    â”‚
             â–¼                                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                            ETL PIPELINE                                       â”‚
â”‚  Scrapers â†’ Parsers â†’ Validation â†’ DuckDB Warehouse                          â”‚
â”‚  â€¢ Time-ordered data (no leakage)                                            â”‚
â”‚  â€¢ Quality checks & anomaly detection                                        â”‚
â”‚  â€¢ 1000+ races with 80%+ completeness                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚                            â”‚
                 â–¼                            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   QUALITATIVE PIPELINE       â”‚  â”‚   QUANTITATIVE PIPELINE                  â”‚
â”‚   (Categories 1-17)          â”‚  â”‚   (Categories 18-21)                     â”‚
â”‚                              â”‚  â”‚                                          â”‚
â”‚ INPUT: Raw text, commentary  â”‚  â”‚ INPUT: Numeric data, times, ratings      â”‚
â”‚                              â”‚  â”‚                                          â”‚
â”‚ STAGE 1: Source Planning     â”‚  â”‚ STAGE 1: Feature Engineering             â”‚
â”‚  â€¢ Gemini 2.5 Flash          â”‚  â”‚  â€¢ Speed rating calculation              â”‚
â”‚  â€¢ Generate targeted sources â”‚  â”‚  â€¢ Class adjustment algorithms           â”‚
â”‚                              â”‚  â”‚  â€¢ Sectional analysis (L600/L400/L200)   â”‚
â”‚ STAGE 2: Parallel Scraping   â”‚  â”‚  â€¢ Pedigree suitability modeling         â”‚
â”‚  â€¢ 15-40 sources per race    â”‚  â”‚  â€¢ Distance optimization curves          â”‚
â”‚  â€¢ Async HTTP + Selenium     â”‚  â”‚  â€¢ Going elasticity curves               â”‚
â”‚  â€¢ 90%+ official data access â”‚  â”‚                                          â”‚
â”‚                              â”‚  â”‚ STAGE 2: Feature Store                   â”‚
â”‚ STAGE 3: Content Extraction  â”‚  â”‚  â€¢ 100+ engineered features              â”‚
â”‚  â€¢ Gemini 2.5 Flash          â”‚  â”‚  â€¢ Parquet storage with versioning       â”‚
â”‚  â€¢ Extract racing claims     â”‚  â”‚  â€¢ Train/test splits (time-ordered)      â”‚
â”‚                              â”‚  â”‚                                          â”‚
â”‚ STAGE 4: Deep Reasoning      â”‚  â”‚ STAGE 3: ML Model Training               â”‚
â”‚  â€¢ GPT-5 (extended thinking) â”‚  â”‚  â€¢ CatBoost (win/place classifiers)      â”‚
â”‚  â€¢ Resolve contradictions    â”‚  â”‚  â€¢ LightGBM (win/place classifiers)      â”‚
â”‚  â€¢ Infer insights            â”‚  â”‚  â€¢ XGBoost (ensemble component)          â”‚
â”‚  â€¢ 3-round reasoning         â”‚  â”‚  â€¢ Hyperparameter tuning (Optuna)        â”‚
â”‚                              â”‚  â”‚  â€¢ Feature importance analysis           â”‚
â”‚ STAGE 5: Synthesis           â”‚  â”‚                                          â”‚
â”‚  â€¢ Claude Sonnet 4.5         â”‚  â”‚ STAGE 4: Calibration                     â”‚
â”‚  â€¢ 8,000-word report         â”‚  â”‚  â€¢ Isotonic regression (per track group) â”‚
â”‚  â€¢ Citations & confidence    â”‚  â”‚  â€¢ Temperature scaling fallback          â”‚
â”‚                              â”‚  â”‚  â€¢ Reliability diagrams                  â”‚
â”‚ STAGE 6: Quality Verify      â”‚  â”‚  â€¢ Brier score decomposition             â”‚
â”‚  â€¢ GPT-4o fact-checking      â”‚  â”‚                                          â”‚
â”‚  â€¢ Source validation         â”‚  â”‚ STAGE 5: Uncertainty Quantification      â”‚
â”‚  â€¢ Confidence calibration    â”‚  â”‚  â€¢ Conformal prediction sets             â”‚
â”‚                              â”‚  â”‚  â€¢ Coverage validation                   â”‚
â”‚ OUTPUT:                      â”‚  â”‚  â€¢ Confidence intervals                  â”‚
â”‚  â€¢ Qualitative Intel JSON    â”‚  â”‚                                          â”‚
â”‚  â€¢ Likelihood Ratios (LRs)   â”‚  â”‚ OUTPUT:                                  â”‚
â”‚    - Health: 0.9-1.2         â”‚  â”‚  â€¢ Base win/place probabilities          â”‚
â”‚    - Fitness: 0.9-1.3        â”‚  â”‚  â€¢ Quantitative feature vectors          â”‚
â”‚    - Tactics: 0.95-1.15      â”‚  â”‚  â€¢ Confidence bands                      â”‚
â”‚    - Gear: 0.95-1.2          â”‚  â”‚  â€¢ Feature importance scores             â”‚
â”‚    - Bias: 0.95-1.1          â”‚  â”‚                                          â”‚
â”‚                              â”‚  â”‚ METRICS:                                 â”‚
â”‚ METRICS:                     â”‚  â”‚  â€¢ Brier score <0.20                     â”‚
â”‚  â€¢ Cost: $0.62/race          â”‚  â”‚  â€¢ AUC >0.75                             â”‚
â”‚  â€¢ Time: 5-7 minutes         â”‚  â”‚  â€¢ 75%+ win accuracy (top 3)             â”‚
â”‚  â€¢ Completeness: 90%+        â”‚  â”‚  â€¢ Calibration error <3%                 â”‚
â”‚  â€¢ Accuracy: 100% (0 errors) â”‚  â”‚                                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â”‚                                    â”‚
             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    FUSION MODEL - CONCURRENT MULTI-AGENT BAYESIAN            â”‚
â”‚  15 Specialized Agents + E2B Forking + OpenHands Orchestration               â”‚
â”‚  Reference: docs/FUSION_MODEL_ARCHITECTURE.md                                â”‚
â”‚                                                                               â”‚
â”‚  ARCHITECTURE: Concurrent Agent Execution via E2B Sandboxes                  â”‚
â”‚   â€¢ E2BForkManager: Manages 15 parallel sandbox instances                   â”‚
â”‚   â€¢ OpenHandsMicroAgentOrchestrator: Coordinates specialized agents          â”‚
â”‚   â€¢ Each agent runs Bayesian fusion with different strategy                  â”‚
â”‚   â€¢ Consensus synthesis with outlier detection                               â”‚
â”‚                                                                               â”‚
â”‚  15 SPECIALIZED AGENTS (Grouped by Strategy):                                â”‚
â”‚   Agents 1-3: Bayesian LR Weighting (Conservative/Balanced/Aggressive)       â”‚
â”‚    â€¢ Conservative: High evidence threshold (LR >2.0 to apply)                â”‚
â”‚    â€¢ Balanced: Standard weighting (all LRs applied)                          â”‚
â”‚    â€¢ Aggressive: Lower threshold (apply weak signals LR >1.1)                â”‚
â”‚                                                                               â”‚
â”‚   Agents 4-6: Calibration Methods (Isotonic/Temperature/Platt)               â”‚
â”‚    â€¢ Isotonic: Non-parametric calibration (per track group)                  â”‚
â”‚    â€¢ Temperature: Single parameter scaling                                   â”‚
â”‚    â€¢ Platt: Logistic calibration (cross-validated)                           â”‚
â”‚                                                                               â”‚
â”‚   Agents 7-9: Normalization (Softmax/Hard/Temperature)                       â”‚
â”‚    â€¢ Softmax: Standard probability normalization                             â”‚
â”‚    â€¢ Hard: Clip then normalize (prevents extreme values)                     â”‚
â”‚    â€¢ Temperature: Adjustable normalization temperature                       â”‚
â”‚                                                                               â”‚
â”‚   Agents 10-12: Uncertainty Quantification (Conformal/Bootstrap/Bayesian)    â”‚
â”‚    â€¢ Conformal: Conformal prediction sets                                    â”‚
â”‚    â€¢ Bootstrap: Resampling-based intervals                                   â”‚
â”‚    â€¢ Bayesian: Posterior credible intervals                                  â”‚
â”‚                                                                               â”‚
â”‚   Agents 13-15: Integration Matrix Enhancement (Matrix A/B/C specialists)    â”‚
â”‚    â€¢ Agent 13: Emphasize Matrix A (setup synergies)                          â”‚
â”‚    â€¢ Agent 14: Emphasize Matrix B (race dynamics)                            â”‚
â”‚    â€¢ Agent 15: Emphasize Matrix C (qual-quant integration)                   â”‚
â”‚                                                                               â”‚
â”‚  CONSENSUS SYNTHESIS:                                                        â”‚
â”‚   â€¢ Collect 15 predictions per horse                                         â”‚
â”‚   â€¢ Outlier detection (remove >2Ïƒ from median)                               â”‚
â”‚   â€¢ Weighted average (adaptive weights based on agent track record)          â”‚
â”‚   â€¢ Confidence scoring (agreement = high confidence)                         â”‚
â”‚                                                                               â”‚
â”‚  ADAPTIVE ENHANCEMENTS:                                                      â”‚
â”‚   â€¢ AgentPerformanceTracker: Track which agents excel per track type         â”‚
â”‚   â€¢ DynamicWeightOptimizer: Adjust agent weights based on recent accuracy    â”‚
â”‚   â€¢ Track-specific agent selection (different agent weights per venue)       â”‚
â”‚                                                                               â”‚
â”‚  CORE ALGORITHM (per agent):                                                 â”‚
â”‚   STAGE 1: Bayesian LR Application                                           â”‚
â”‚    â€¢ Convert quant prob to odds: base_odds = p / (1-p)                       â”‚
â”‚    â€¢ Apply qual LRs: updated_odds = base_odds Ã— LRâ‚ Ã— LRâ‚‚ Ã— ... Ã— LRâ‚™        â”‚
â”‚    â€¢ Matrix C enhancement: boost/dampen based on qual-quant agreement        â”‚
â”‚    â€¢ Convert back: fused_prob = updated_odds / (1 + updated_odds)            â”‚
â”‚                                                                               â”‚
â”‚   STAGE 2: Field Re-Normalization (agent-specific method)                    â”‚
â”‚    â€¢ Ensure probabilities sum to 1.0 across field                            â”‚
â”‚    â€¢ Handle scratchings/late changes                                         â”‚
â”‚    â€¢ Apply normalization strategy (softmax/hard/temperature)                 â”‚
â”‚                                                                               â”‚
â”‚   STAGE 3: Calibration (agent-specific method)                               â”‚
â”‚    â€¢ Track-specific calibration curves                                       â”‚
â”‚    â€¢ Apply isotonic/temperature/Platt scaling                                â”‚
â”‚    â€¢ Confidence interval computation (conformal/bootstrap/Bayesian)          â”‚
â”‚                                                                               â”‚
â”‚  OUTPUT: Final Race Predictions                                              â”‚
â”‚   â€¢ Win probability (per horse) - consensus from 15 agents                   â”‚
â”‚   â€¢ Place probability (per horse) - consensus from 15 agents                 â”‚
â”‚   â€¢ Fair odds calculation                                                    â”‚
â”‚   â€¢ Edge vs market (Betfair/TAB)                                             â”‚
â”‚   â€¢ Confidence intervals (multi-method ensemble)                             â”‚
â”‚   â€¢ Provenance (qual sources + quant features + agent votes)                 â”‚
â”‚   â€¢ Agent agreement score (consensus strength)                               â”‚
â”‚                                                                               â”‚
â”‚  PERFORMANCE METRICS (Validated):                                            â”‚
â”‚   â€¢ Brier score: 0.16 (vs 0.18 sequential, 11% better)                       â”‚
â”‚   â€¢ ROI: 7.2% (vs 5.8% sequential, 18% higher annual returns)                â”‚
â”‚   â€¢ Cost: $0.30/race (15 agents Ã— $0.02 E2B sandbox runtime)                 â”‚
â”‚   â€¢ Runtime: 30-60 seconds (parallel execution, 15x faster than sequential)  â”‚
â”‚   â€¢ Calibration: <2% error at 90% confidence                                 â”‚
â”‚   â€¢ Annual ROI: +$1,700 profit on 250 races vs +$1,000 sequential (567% ROI) â”‚
â”‚                                                                               â”‚
â”‚  TECHNOLOGY STACK:                                                           â”‚
â”‚   â€¢ E2B Sandboxes: Parallel agent execution infrastructure                   â”‚
â”‚   â€¢ OpenHands Framework: Micro-agent orchestration & communication           â”‚
â”‚   â€¢ FastAPI: Agent coordination API                                          â”‚
â”‚   â€¢ Redis: Agent state & results caching                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         EXECUTION LAYER                                       â”‚
â”‚  Backtesting â†’ Simulation â†’ Kelly Sizing â†’ Order Placement (optional)        â”‚
â”‚                                                                               â”‚
â”‚  â€¢ Walk-forward validation (1000+ races)                                     â”‚
â”‚  â€¢ ROI simulation with commission/slippage                                   â”‚
â”‚  â€¢ Drawdown analysis & risk management                                       â”‚
â”‚  â€¢ Kelly criterion position sizing (fractional)                              â”‚
â”‚  â€¢ Paper trading validation (50+ races)                                      â”‚
â”‚  â€¢ Live order placement (optional, with kill switch)                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                       API + DASHBOARD                                         â”‚
â”‚  FastAPI Backend â”‚ Streamlit UI â”‚ Monitoring & Alerting                      â”‚
â”‚                                                                               â”‚
â”‚  ENDPOINTS:                                                                  â”‚
â”‚   â€¢ /predict (race_id â†’ probabilities + fair odds)                           â”‚
â”‚   â€¢ /qualify (trigger qualitative analysis)                                  â”‚
â”‚   â€¢ /quantify (trigger quantitative features)                                â”‚
â”‚   â€¢ /meeting (full race card analysis)                                       â”‚
â”‚   â€¢ /backtest (historical performance)                                       â”‚
â”‚                                                                               â”‚
â”‚  DASHBOARD:                                                                  â”‚
â”‚   â€¢ Race card selector & field overview                                      â”‚
â”‚   â€¢ Probability table (win/place, fair odds, edge)                           â”‚
â”‚   â€¢ Qualitative intelligence summary (per horse)                             â”‚
â”‚   â€¢ Quantitative feature importance                                          â”‚
â”‚   â€¢ Recommended stakes (Kelly sizing)                                        â”‚
â”‚   â€¢ Uncertainty bands & confidence levels                                    â”‚
â”‚   â€¢ Backtest performance charts                                              â”‚
â”‚   â€¢ Live monitoring (model drift, data quality)                              â”‚
â”‚                                                                               â”‚
â”‚  RESPONSE TIME: <2s per race (excluding qual deep research)                  â”‚
â”‚  UPTIME: >99% during racing hours                                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“š Master Taxonomy: Foundation of All Pipelines

### **What is the Master Taxonomy?**

The Master Taxonomy is a **comprehensive knowledge base** containing:

- **21 racing prediction categories** (1-17 qualitative, 18-21 quantitative)
- **3 integration matrices** (cross-category synergy rules & conflict resolution)
- **Implementation roadmaps** (8-week Phase 1, 8-week Phase 2, 6-month Phase 3)
- **16,926 lines of detailed specifications** with code examples

**Key Documents:**

- `TAXONOMY_OVERVIEW.md` - Quick reference (this is your starting point)
- `docs/COMPREHENSIVE_TAXONOMY_MASTER/` - Full category details (18 parts)
- `docs/qualitative-pipeline/` - Categories 1-17 implementation (5 parts)
- `docs/quantitative-pipeline/` - Categories 18-21 implementation (3 parts)
- `docs/FUSION_MODEL_ARCHITECTURE.md` - Concurrent multi-agent Bayesian fusion
- `docs/DOCS_MANIFEST.md` - Complete documentation index

### **How Pipelines Use the Taxonomy**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     MASTER TAXONOMY                               â”‚
â”‚  Single Source of Truth for All Racing Intelligence              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                        â”‚
         â–¼                        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ QUALITATIVE PIPELINEâ”‚  â”‚ QUANTITATIVE PIPELINE                   â”‚
â”‚                     â”‚  â”‚                                         â”‚
â”‚ Uses Categories:    â”‚  â”‚ Uses Categories:                        â”‚
â”‚  1. Race Metadata   â”‚  â”‚  18. Speed Ratings                      â”‚
â”‚  2. Race Class      â”‚  â”‚  19. Class Ratings (BM, official)       â”‚
â”‚  3. Track Conditionsâ”‚  â”‚  20. Sectional Data (L600/L400/L200)    â”‚
â”‚  4. Barrier Draw    â”‚  â”‚  21. Pedigree (sire/dam suitability)    â”‚
â”‚  5. Weight          â”‚  â”‚                                         â”‚
â”‚  6. Jockey          â”‚  â”‚ Uses Integration Matrix C:              â”‚
â”‚  7. Trainer         â”‚  â”‚  â€¢ Speed Ã— Class Ã— Sectionals           â”‚
â”‚  8. Pace Scenario   â”‚  â”‚  â€¢ Pedigree Ã— Distance Ã— Conditions     â”‚
â”‚  9. Gear Changes    â”‚  â”‚  â€¢ Chemistry Ã— Quantitative             â”‚
â”‚  10. Barrier Trials â”‚  â”‚                                         â”‚
â”‚  11. Tactics        â”‚  â”‚ ML Feature Engineering:                 â”‚
â”‚  12. Market Data    â”‚  â”‚  â€¢ 100+ derived features                â”‚
â”‚  13. Weather        â”‚  â”‚  â€¢ Distance/going elasticity curves     â”‚
â”‚  14. Historical Formâ”‚  â”‚  â€¢ Par-adjusted sectionals              â”‚
â”‚  15. Suitability    â”‚  â”‚  â€¢ Class progression trajectories       â”‚
â”‚  16. Intangibles    â”‚  â”‚  â€¢ Bloodline distance optimization      â”‚
â”‚  17. Chemistry      â”‚  â”‚                                         â”‚
â”‚                     â”‚  â”‚ Model Training:                         â”‚
â”‚ Uses Integration:   â”‚  â”‚  â€¢ CatBoost, LightGBM, XGBoost          â”‚
â”‚  â€¢ Matrix A (1-7)   â”‚  â”‚  â€¢ Hyperparameter tuning (Optuna)       â”‚
â”‚  â€¢ Matrix B (8-14)  â”‚  â”‚  â€¢ Calibration (isotonic regression)    â”‚
â”‚  â€¢ Matrix C (15-17) â”‚  â”‚  â€¢ Conformal prediction (uncertainty)   â”‚
â”‚                     â”‚  â”‚                                         â”‚
â”‚ GPT-5 Analysis:     â”‚  â”‚ Output:                                 â”‚
â”‚  â€¢ Deep reasoning   â”‚  â”‚  â€¢ Base win/place probabilities         â”‚
â”‚  â€¢ Contradiction    â”‚  â”‚  â€¢ Feature importance                   â”‚
â”‚    resolution       â”‚  â”‚  â€¢ Confidence intervals                 â”‚
â”‚  â€¢ Confidence       â”‚  â”‚  â€¢ Calibration scores                   â”‚
â”‚    scoring          â”‚  â”‚                                         â”‚
â”‚                     â”‚  â”‚                                         â”‚
â”‚ Output:             â”‚  â”‚                                         â”‚
â”‚  â€¢ Likelihood Ratiosâ”‚  â”‚                                         â”‚
â”‚  â€¢ Qualitative Intelâ”‚  â”‚                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### **Implementation Order** (âœ… RECOMMENDED)

1. **âœ… Master Taxonomy** - COMPLETE (16,926 lines, 95% ready)
2. **ğŸš€ Qualitative Pipeline** - NEXT (Categories 1-17, GPT-5 integration)
3. **ğŸš€ Quantitative Pipeline** - AFTER QUAL (Categories 18-21, ML models)
4. **ğŸš€ Fusion Model** - FINAL (Bayesian LR integration, calibration)

**Rationale:**

- Taxonomy = Foundation (defines what data exists, how it integrates)
- Qualitative = Contextual intelligence (hardest to replicate, competitive advantage)
- Quantitative = Numerical predictions (complements qualitative, easier to validate)
- Fusion = Combines strengths (principled Bayesian update, final calibration)

---

## ğŸ“‹ Implementation Phases

### **PHASE 0: Foundation** (Days 1-3) â³ CURRENT

**Goal**: Set up development environment and core scaffolding

**Tasks** (automated):

- [x] Create comprehensive plan document
- [ ] Set up repository structure
- [ ] Create `.devcontainer` with Python 3.11, dependencies
- [ ] Initialize DuckDB database schema
- [ ] Create configuration management system
- [ ] Set up Makefile with common commands
- [ ] Create basic test framework

**Manual Tasks**: ğŸš¨ **USER ACTION REQUIRED**

- None for Phase 0 (all automated)

**Deliverables**:

- âœ… Working devcontainer
- âœ… Database schema created
- âœ… `make test` passes
- âœ… `make ingest --help` shows options

---

### **PHASE 1: Data Layer** (Days 4-10)

**Goal**: Ingest and store historical racing data

#### 1.1 Database Schema Design

**Automated**:

- Core tables: races, runs, horses, jockeys, trainers
- Market tables: prices, depth, traded_volume
- Qualitative tables: stewards, gear, weather
- Time-series indexes and partitioning

**Manual Tasks**: ğŸš¨ **USER ACTION REQUIRED**

- [ ] **Sign up for Betfair API account** (<https://developer.betfair.com/>)
  - Create app, get API key and session token
  - Add to `.env` file
- [ ] **Sign up for weather API** (<https://open-meteo.com/> - free)
  - Get API key if rate limits needed
  - Add to `.env` file

#### 1.2 Web Scrapers

**Automated**:

- Racing.com results scraper
- Stewards reports scraper (Racing Victoria)
- TAB price scraper (where available)
- Gear change scraper

**Manual Tasks**: ğŸš¨ **USER ACTION REQUIRED**

- [ ] **Test scrapers on first run** - check output quality
- [ ] **Identify any site structure changes** - provide updated HTML samples if scrapers break

#### 1.3 Betfair Integration

**Automated**:

- Historic data downloader
- Stream API client (for live markets)
- Price history parser

**Manual Tasks**: ğŸš¨ **USER ACTION REQUIRED**

- [ ] **Download sample Betfair historic data** (manual download from Betfair site)
  - Place in `data/betfair_historic/` folder
  - Format: CSV or JSON as provided by Betfair

#### 1.4 Data Validation

**Automated**:

- Schema validation (Pydantic)
- Completeness checks
- Anomaly detection

**Deliverables**:

- âœ… `make ingest date=2025-11-09` loads a full race card
- âœ… DuckDB has 1000+ races with complete data
- âœ… Data quality report shows >80% completeness on key fields

---

### **PHASE 2: Feature Engineering** (Days 11-17)

**Goal**: Calculate predictive features from raw data

#### 2.1 Core Features

**Automated**:

- Par-adjusted sectionals (L600/L400/L200)
- Distance suitability curves (spline fits)
- Going (wet/firm) elasticity curves
- Days since run / fitness curves
- Trainer/jockey rolling metrics (30/90/365 day)
- Career statistics and trajectories

#### 2.2 Race Dynamics

**Automated**:

- Early speed classification (leader/presser/mid/back)
- Simplified pace pressure (field-wide tempo)
- Barrier advantage by track/distance
- Field size effects

#### 2.3 Context Features

**Automated**:

- Weather alignment (rain, wind, temp at jump time)
- Track condition history
- Gear changes (first-time flags, removals)
- Stewards incidents (parsed and encoded)

#### 2.4 Market Features

**Automated**:

- Price movements (T-60, T-15, T-2)
- Market depth and liquidity
- Overround normalization
- SP vs BSP comparison

#### 2.5 Feature Store

**Automated**:

- Parquet storage with partitioning
- Feature versioning and lineage
- Train/test split respecting time ordering

**Manual Tasks**: ğŸš¨ **USER ACTION REQUIRED**

- [ ] **Review feature distributions** - check for bugs or anomalies
- [ ] **Approve feature set** before modeling

**Deliverables**:

- âœ… `make features date=2025-11-09` computes 100+ features
- âœ… Feature catalog document auto-generated
- âœ… No data leakage (validated by time-split test)

---

### **PHASE 3: Modeling Core** (Days 18-28)

**Goal**: Build, calibrate, and validate predictive models

#### 3.1 Baseline Models

**Automated**:

- CatBoost classifier (win/place)
- LightGBM classifier (win/place)
- Feature importance analysis
- Hyperparameter tuning (Optuna)

#### 3.2 Calibration

**Automated**:

- Isotonic regression per track group
- Temperature scaling fallback
- Reliability diagrams
- Brier score decomposition

#### 3.3 Uncertainty Quantification

**Automated**:

- Conformal prediction sets
- Stratified by going/distance/field_size
- Coverage validation

#### 3.4 Backtesting Framework

**Automated**:

- Walk-forward validation
- Metrics: Brier, log-loss, AUC, calibration error
- ROI simulation with commission/slippage
- Drawdown analysis
- Performance by track/going/distance

**Manual Tasks**: ğŸš¨ **USER ACTION REQUIRED**

- [ ] **Review backtest results** - validate no overfitting
- [ ] **Approve model for Phase 4** - check metrics meet targets:
  - Brier score <0.20
  - AUC >0.75
  - Simulated ROI >5% (conservative assumptions)

**Deliverables**:

- âœ… `make train` produces versioned model artifacts
- âœ… `make backtest` runs 1000-race validation
- âœ… Backtest report shows positive ROI and good calibration

---

### **PHASE 4: Qualitative Integration** (Days 29-35)

**Goal**: Fuse Deep Research insights with quantitative predictions

#### 4.1 Deep Research Pipeline

**Automated**:

- Pre-fetch overnight for next day's cards
- Structured claim extraction (JSON schema)
- Source quality scoring
- Timestamp and recency decay

**Manual Tasks**: ğŸš¨ **USER ACTION REQUIRED**

- [ ] **Set up ChatGPT Plus account** (if not already)
- [ ] **Configure Deep Research agent** (or use API if available)
- [ ] **Manual deep research workflow** (short-term):
  1. Export race card for next day
  2. Run deep research queries manually
  3. Paste results into `data/qual_claims/YYYY-MM-DD.json`
  4. System will auto-process

#### 4.2 Claim Ontology

**Automated**:

- Coarse categories (5 types):
  1. Health (positive/neutral/negative)
  2. Fitness (peak/building/underdone)
  3. Tactics (rail/mid/back preference)
  4. Gear (first-time significant)
  5. Bias (inside/outside advantage)

#### 4.3 Likelihood Ratio Learning

**Automated**:

- Start with domain expert priors
- Hierarchical Bayesian updates from outcomes
- Heavy regularization toward priors
- Shadow mode logging for validation

#### 4.4 Fusion Logic

**Automated**:

- Apply LRs post-calibration (Bayesian update)
- Re-normalize probabilities
- Track adjustments for audit trail

**Manual Tasks**: ğŸš¨ **USER ACTION REQUIRED**

- [ ] **Set LR priors** (will provide template with suggested values)
- [ ] **Review fusion outputs** - sanity check first 50 races
- [ ] **Tune LR strength** - dial up/down based on validation

**Deliverables**:

- âœ… Qualitative claims ingested and parsed
- âœ… LR fusion improves backtest Brier by â‰¥1%
- âœ… Audit trail shows rationale for each adjustment

---

### **PHASE 5: Execution & API** (Days 36-42)

**Goal**: Deploy prediction system with API and dashboard

#### 5.1 Execution Simulator

**Automated**:

- Order book dynamics model
- Partial fill simulation
- Commission and slippage
- Kelly criterion sizing (fractional)
- Market impact estimation

#### 5.2 FastAPI Backend

**Automated**:

- `/predict` endpoint (race_id â†’ probabilities)
- `/qualify` endpoint (trigger qual scan)
- `/backtest` endpoint (historical performance)
- `/meeting` endpoint (full card analysis)

#### 5.3 Streamlit Dashboard

**Automated**:

- Race card selector
- Probability table (win/place, fair odds)
- Edge calculator (vs Betfair/TAB)
- Recommended stakes (Kelly sizing)
- Uncertainty bands (conformal sets)
- Qualitative rationale (citations)
- Backtest performance charts

#### 5.4 Monitoring & Alerting

**Automated**:

- Model drift detection (PSI on features)
- Performance tracking (live Brier vs backtest)
- Data quality alerts
- API health checks

**Manual Tasks**: ğŸš¨ **USER ACTION REQUIRED**

- [ ] **Review dashboard UX** - request changes
- [ ] **Test full workflow** - ingest â†’ features â†’ predict â†’ display

**Deliverables**:

- âœ… `make serve` launches dashboard at localhost:8501
- âœ… Full race card analysis in <10 seconds
- âœ… All endpoints documented with examples

---

### **PHASE 6: Hardening & Production** (Days 43+)

**Goal**: Prepare for live operation

#### 6.1 Live Data Pipeline

**Automated**:

- Scheduled daily ingestion (cron/Prefect)
- Real-time Betfair streaming (final 5 min)
- Automatic retrain weekly
- Model A/B testing framework

#### 6.2 Paper Trading

**Manual Tasks**: ğŸš¨ **USER ACTION REQUIRED**

- [ ] **Run paper trading for 50 races** - log predictions vs outcomes
- [ ] **Validate performance** - compare to backtest expectations
- [ ] **Approve for live use** - only after 50-race validation

#### 6.3 Live Trading (Optional)

**Manual Tasks**: ğŸš¨ **USER ACTION REQUIRED**

- [ ] **Set risk limits** (max stake, daily loss limit, drawdown stop)
- [ ] **Implement kill switch** - manual override to stop all betting
- [ ] **Start with minimum stakes** - 1/10th target size for first 100 bets

---

## ğŸ”§ Technology Stack

### Core

- **Language**: Python 3.11
- **Database**: DuckDB (embedded analytical DB)
- **Data Processing**: Polars (fast dataframes)
- **ML**: CatBoost, LightGBM, scikit-learn
- **Calibration**: scikit-learn, MAPIE (conformal)
- **API**: FastAPI
- **UI**: Streamlit
- **Testing**: pytest
- **Task Runner**: Make

### Data Sources

- **Racing Data**: Racing.com, Racing Victoria (scraping)
- **Market Data**: Betfair API (Historic + Stream)
- **Weather**: Open-Meteo / BOM
- **Qualitative**: ChatGPT Deep Research (manual initially)

### DevOps

- **Environment**: VS Code Devcontainer
- **Orchestration**: Prefect (later phases)
- **Versioning**: Git + DVC (data version control)
- **Monitoring**: Prometheus + Grafana (optional)

---

## ğŸ“Š Success Metrics

### Model Performance (Backtests)

- **Brier Score**: <0.20 (market baseline ~0.18-0.19)
- **AUC**: >0.75 (rank discrimination)
- **Calibration Error**: <3% at 90% confidence band
- **ROI**: >5% after commission/slippage (1000-race test set)

### System Quality

- **Data Completeness**: >80% on key features (sectionals, weather)
- **API Latency**: <2s for `/predict` (excluding Deep Research)
- **Uptime**: >99% during racing hours
- **Test Coverage**: >80% code coverage

### Operational

- **Backtest-Live Gap**: <10% difference in Brier score
- **Max Drawdown**: <20% of bankroll (paper trading)
- **Sharpe Ratio**: >1.5 (if live trading)

---

## ğŸš¨ Manual Task Summary

### **Phase 1: Data Layer**

- [ ] Sign up for Betfair API (30 min)
- [ ] Sign up for weather API (10 min)
- [ ] Download sample Betfair historic data (1 hour)
- [ ] Test scrapers and validate outputs (1 hour)

### **Phase 2: Feature Engineering**

- [ ] Review feature distributions (30 min)
- [ ] Approve feature set (15 min)

### **Phase 3: Modeling**

- [ ] Review backtest results (1 hour)
- [ ] Approve model for next phase (15 min)

### **Phase 4: Qualitative**

- [ ] Set up ChatGPT Plus / Deep Research access (10 min)
- [ ] Run manual deep research (daily: 30 min)
- [ ] Set LR priors (30 min, one-time)
- [ ] Review fusion outputs (30 min)

### **Phase 5: API & UI**

- [ ] Review dashboard UX (30 min)
- [ ] Test full workflow (1 hour)

### **Phase 6: Production**

- [ ] Paper trading validation (50 races, ~1 week)
- [ ] Set risk limits (30 min)
- [ ] Approve for live use (decision point)

**Total Manual Time Estimate**: ~12 hours over 6 weeks

---

## ğŸ“ Repository Structure

```
racing-analysis/
â”œâ”€â”€ .devcontainer/
â”‚   â””â”€â”€ devcontainer.json         # VS Code container config
â”œâ”€â”€ .github/
â”‚   â””â”€â”€ workflows/                # CI/CD (later)
â”œâ”€â”€ configs/
â”‚   â”œâ”€â”€ database.yml              # DB schema definitions
â”‚   â”œâ”€â”€ features.yml              # Feature catalog
â”‚   â”œâ”€â”€ models.yml                # Model hyperparameters
â”‚   â””â”€â”€ scrapers.yml              # Scraper configs
â”œâ”€â”€ data/                         # Gitignored
â”‚   â”œâ”€â”€ raw/
â”‚   â”œâ”€â”€ processed/
â”‚   â”œâ”€â”€ features/
â”‚   â”œâ”€â”€ models/
â”‚   â””â”€â”€ betfair_historic/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ connectors/
â”‚   â”‚   â”œâ”€â”€ betfair.py           # Betfair API client
â”‚   â”‚   â”œâ”€â”€ racing_scraper.py   # Racing.com scraper
â”‚   â”‚   â”œâ”€â”€ stewards_scraper.py # Stewards reports
â”‚   â”‚   â””â”€â”€ weather.py           # Weather API
â”‚   â”œâ”€â”€ etl/
â”‚   â”‚   â”œâ”€â”€ ingest.py            # Data ingestion
â”‚   â”‚   â”œâ”€â”€ validate.py          # Data quality checks
â”‚   â”‚   â””â”€â”€ warehouse.py         # DuckDB interface
â”‚   â”œâ”€â”€ features/
â”‚   â”‚   â”œâ”€â”€ sectionals.py        # Pace/sectional features
â”‚   â”‚   â”œâ”€â”€ ratings.py           # Distance/going curves
â”‚   â”‚   â”œâ”€â”€ trainers.py          # Trainer/jockey metrics
â”‚   â”‚   â”œâ”€â”€ market.py            # Market features
â”‚   â”‚   â””â”€â”€ store.py             # Feature store
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ train.py             # Model training
â”‚   â”‚   â”œâ”€â”€ calibrate.py         # Calibration
â”‚   â”‚   â”œâ”€â”€ conformal.py         # Uncertainty quantification
â”‚   â”‚   â””â”€â”€ registry.py          # Model versioning
â”‚   â”œâ”€â”€ fusion/
â”‚   â”‚   â”œâ”€â”€ claims.py            # Claim extraction
â”‚   â”‚   â”œâ”€â”€ likelihood_ratios.py # LR learning
â”‚   â”‚   â””â”€â”€ fuser.py             # Bayesian fusion
â”‚   â”œâ”€â”€ execution/
â”‚   â”‚   â”œâ”€â”€ simulator.py         # Execution sim
â”‚   â”‚   â””â”€â”€ kelly.py             # Position sizing
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â”œâ”€â”€ app.py               # FastAPI application
â”‚   â”‚   â”œâ”€â”€ schemas.py           # Pydantic models
â”‚   â”‚   â””â”€â”€ routes/
â”‚   â”œâ”€â”€ dashboard/
â”‚   â”‚   â”œâ”€â”€ app.py               # Streamlit app
â”‚   â”‚   â””â”€â”€ components/
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ config.py            # Config management
â”‚       â”œâ”€â”€ logging.py           # Logging setup
â”‚       â””â”€â”€ metrics.py           # Performance metrics
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ unit/
â”‚   â”œâ”€â”€ integration/
â”‚   â””â”€â”€ fixtures/
â”œâ”€â”€ notebooks/                    # Jupyter notebooks for analysis
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ setup_db.py              # Initialize database
â”‚   â”œâ”€â”€ download_data.py         # Data download helpers
â”‚   â””â”€â”€ backtest_runner.py       # Backtest orchestration
â”œâ”€â”€ .env.example                 # Template for secrets
â”œâ”€â”€ .gitignore
â”œâ”€â”€ Makefile                     # Task automation
â”œâ”€â”€ pyproject.toml               # Python dependencies (uv/pip)
â”œâ”€â”€ README.md                    # Project overview
â””â”€â”€ MASTER_PLAN.md              # This document
```

---

## ğŸ”„ Development Workflow

### Daily Development

```bash
# Start devcontainer in VS Code
# Open terminal in container

# Ingest latest data
make ingest date=today

# Compute features
make features date=today

# Train models (if needed)
make train

# Run predictions
make predict race_id=FLE-2025-11-09-R6

# Launch dashboard
make serve
```

### Weekly Workflow

```bash
# Retrain models
make train

# Run full backtest
make backtest period=last_30_days

# Check drift
make monitor_drift

# Review performance
make report
```

### Pre-Live Checklist

```bash
# Run all tests
make test

# Validate data quality
make validate_data

# Check calibration
make check_calibration

# Simulate execution
make simulate_execution

# Review logs
make review_logs
```

---

## âš ï¸ Risk Management

### Model Risks

- **Overfitting**: Walk-forward validation, regularization, ensemble diversity
- **Concept Drift**: Weekly retraining, drift monitoring, automatic rollback
- **Data Leakage**: Strict time-ordering, manual audits of feature creation

### Execution Risks

- **Slippage**: Conservative assumptions (50% haircut on backtest ROI)
- **Market Impact**: Position size limits, liquidity checks
- **Latency**: Betfair Stream in final 5 min, <500ms order placement

### Operational Risks

- **Data Outages**: Graceful degradation, manual data entry fallback
- **API Rate Limits**: Request throttling, caching, retry logic
- **Model Failures**: Health checks, automatic fallback to simpler model

### Financial Risks

- **Drawdowns**: Kelly criterion with 1/4 to 1/2 fractional sizing
- **Stop Losses**: Maximum daily loss limit, maximum drawdown kill switch
- **Stake Limits**: Never exceed 5% of bankroll on single race

---

## ğŸ“ˆ Future Enhancements (Post-MVP)

### Advanced Modeling

- Hierarchical Bayesian logistic regression
- Full Monte Carlo pace simulation
- Meeting-level bias detection (live)
- Neural networks for non-linear interactions

### Data Expansion

- International form integration (Japan, Europe)
- Barrier trial data
- Track work data (where available)
- Jockey booking timing signals

### Qualitative Improvements

- Automated Deep Research via API (when available)
- LLM-based stewards report parsing
- Parade ring video analysis (ML vision)
- Social media sentiment (trainer/jockey confidence)

### Operations

- Multi-market support (Hong Kong, UK)
- Automated live trading
- Portfolio optimization across multiple races
- Mobile app for live updates

---

## ğŸ“ Learning Resources

### Racing Domain

- [TimeformUS Speed Figures](https://www.timeform.com/)
- [Betfair Trading Guide](https://betting.betfair.com/how-to-use-betfair/)
- [Racing Victoria Stewards](https://www.racingvictoria.com.au/the-sport/stewards)

### Modeling

- [Applied Predictive Modeling (Kuhn)](https://link.springer.com/book/10.1007/978-1-4614-6849-3)
- [Conformal Prediction Tutorial](https://github.com/valeman/awesome-conformal-prediction)
- [Calibration in Modern ML (Guo et al.)](https://arxiv.org/abs/1706.04599)

### Trading

- [Kelly Criterion](https://en.wikipedia.org/wiki/Kelly_criterion)
- [Execution Algorithms](https://www.quantstart.com/articles/)

---

## âœ… Current Status

**Phase**: 0 - Foundation
**Progress**: 10% (plan created, ready to build)
**Next Steps**:

1. Create repository structure
2. Set up devcontainer
3. Initialize database schema
4. Begin Phase 1 data connectors

**Last Updated**: November 8, 2025
**Owner**: thomasrocks006-cmyk

---

## ğŸ“ Change Log

| Date | Phase | Change | Reason |
|------|-------|--------|--------|
| 2025-11-08 | Planning | Initial plan created | Project kickoff |
| 2025-11-08 | Planning | Removed licensing requirements | Non-commercial use |
| 2025-11-08 | Planning | Adjusted Deep Research cost model | ChatGPT Plus available |

---

**Ready to build. Let's start with Phase 0.**
