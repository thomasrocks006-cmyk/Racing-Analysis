# Racing Analysis System - Master Implementation Plan

**Created**: November 8, 2025  
**Status**: Phase 0 - Foundation  
**Goal**: Build a high-accuracy horse racing prediction system with quantitative modeling + qualitative research fusion

---

## ğŸ¯ Project Goals

1. **Primary**: Produce calibrated win/place probabilities and fair odds per race
2. **Secondary**: Identify value opportunities vs market prices (Betfair/TAB)
3. **Tertiary**: Build reproducible backtesting framework with uncertainty quantification

**Non-Negotiables**:
- âœ… Strict event-time ordering (no data leakage)
- âœ… Reproducible runs with versioning
- âœ… Uncertainty quantification on all predictions
- âœ… Non-commercial personal use (scraping permitted)

---

## ğŸ—ï¸ System Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         DATA SOURCES                             â”‚
â”‚  Betfair (Historic/Stream) â”‚ Racing.com â”‚ Weather â”‚ Stewards    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      ETL PIPELINE                                â”‚
â”‚  Scrapers â†’ Parsers â†’ Validation â†’ DuckDB Warehouse             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   FEATURE STORE (Parquet)                        â”‚
â”‚  Sectionals â”‚ Ratings â”‚ Pace â”‚ Bias â”‚ Weather â”‚ Market          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      MODELING LAYER                              â”‚
â”‚  CatBoost/LightGBM â†’ Calibration â†’ Conformal Prediction         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   QUALITATIVE FUSION                             â”‚
â”‚  Deep Research â†’ Claim Extraction â†’ Bayesian LR Update          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    EXECUTION LAYER                               â”‚
â”‚  Simulator â†’ Kelly Sizing â†’ Order Placement                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     API + DASHBOARD                              â”‚
â”‚  FastAPI Backend â”‚ Streamlit UI â”‚ Monitoring                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“‹ Implementation Phases

### **PHASE 0: Foundation** (Days 1-3) â³ CURRENT
**Goal**: Set up development environment and core scaffolding

**Tasks** (automated):
- [x] Create comprehensive plan document
- [ ] Set up repository structure
- [ ] Create `.devcontainer` with Python 3.11, dependencies
- [ ] Initialize DuckDB database schema
- [ ] Create configuration management system
- [ ] Set up Makefile with common commands
- [ ] Create basic test framework

**Manual Tasks**: ğŸš¨ **USER ACTION REQUIRED**
- None for Phase 0 (all automated)

**Deliverables**:
- âœ… Working devcontainer
- âœ… Database schema created
- âœ… `make test` passes
- âœ… `make ingest --help` shows options

---

### **PHASE 1: Data Layer** (Days 4-10)
**Goal**: Ingest and store historical racing data

#### 1.1 Database Schema Design
**Automated**:
- Core tables: races, runs, horses, jockeys, trainers
- Market tables: prices, depth, traded_volume
- Qualitative tables: stewards, gear, weather
- Time-series indexes and partitioning

**Manual Tasks**: ğŸš¨ **USER ACTION REQUIRED**
- [ ] **Sign up for Betfair API account** (https://developer.betfair.com/)
  - Create app, get API key and session token
  - Add to `.env` file
- [ ] **Sign up for weather API** (https://open-meteo.com/ - free)
  - Get API key if rate limits needed
  - Add to `.env` file

#### 1.2 Web Scrapers
**Automated**:
- Racing.com results scraper
- Stewards reports scraper (Racing Victoria)
- TAB price scraper (where available)
- Gear change scraper

**Manual Tasks**: ğŸš¨ **USER ACTION REQUIRED**
- [ ] **Test scrapers on first run** - check output quality
- [ ] **Identify any site structure changes** - provide updated HTML samples if scrapers break

#### 1.3 Betfair Integration
**Automated**:
- Historic data downloader
- Stream API client (for live markets)
- Price history parser

**Manual Tasks**: ğŸš¨ **USER ACTION REQUIRED**
- [ ] **Download sample Betfair historic data** (manual download from Betfair site)
  - Place in `data/betfair_historic/` folder
  - Format: CSV or JSON as provided by Betfair

#### 1.4 Data Validation
**Automated**:
- Schema validation (Pydantic)
- Completeness checks
- Anomaly detection

**Deliverables**:
- âœ… `make ingest date=2025-11-09` loads a full race card
- âœ… DuckDB has 1000+ races with complete data
- âœ… Data quality report shows >80% completeness on key fields

---

### **PHASE 2: Feature Engineering** (Days 11-17)
**Goal**: Calculate predictive features from raw data

#### 2.1 Core Features
**Automated**:
- Par-adjusted sectionals (L600/L400/L200)
- Distance suitability curves (spline fits)
- Going (wet/firm) elasticity curves
- Days since run / fitness curves
- Trainer/jockey rolling metrics (30/90/365 day)
- Career statistics and trajectories

#### 2.2 Race Dynamics
**Automated**:
- Early speed classification (leader/presser/mid/back)
- Simplified pace pressure (field-wide tempo)
- Barrier advantage by track/distance
- Field size effects

#### 2.3 Context Features
**Automated**:
- Weather alignment (rain, wind, temp at jump time)
- Track condition history
- Gear changes (first-time flags, removals)
- Stewards incidents (parsed and encoded)

#### 2.4 Market Features
**Automated**:
- Price movements (T-60, T-15, T-2)
- Market depth and liquidity
- Overround normalization
- SP vs BSP comparison

#### 2.5 Feature Store
**Automated**:
- Parquet storage with partitioning
- Feature versioning and lineage
- Train/test split respecting time ordering

**Manual Tasks**: ğŸš¨ **USER ACTION REQUIRED**
- [ ] **Review feature distributions** - check for bugs or anomalies
- [ ] **Approve feature set** before modeling

**Deliverables**:
- âœ… `make features date=2025-11-09` computes 100+ features
- âœ… Feature catalog document auto-generated
- âœ… No data leakage (validated by time-split test)

---

### **PHASE 3: Modeling Core** (Days 18-28)
**Goal**: Build, calibrate, and validate predictive models

#### 3.1 Baseline Models
**Automated**:
- CatBoost classifier (win/place)
- LightGBM classifier (win/place)
- Feature importance analysis
- Hyperparameter tuning (Optuna)

#### 3.2 Calibration
**Automated**:
- Isotonic regression per track group
- Temperature scaling fallback
- Reliability diagrams
- Brier score decomposition

#### 3.3 Uncertainty Quantification
**Automated**:
- Conformal prediction sets
- Stratified by going/distance/field_size
- Coverage validation

#### 3.4 Backtesting Framework
**Automated**:
- Walk-forward validation
- Metrics: Brier, log-loss, AUC, calibration error
- ROI simulation with commission/slippage
- Drawdown analysis
- Performance by track/going/distance

**Manual Tasks**: ğŸš¨ **USER ACTION REQUIRED**
- [ ] **Review backtest results** - validate no overfitting
- [ ] **Approve model for Phase 4** - check metrics meet targets:
  - Brier score <0.20
  - AUC >0.75
  - Simulated ROI >5% (conservative assumptions)

**Deliverables**:
- âœ… `make train` produces versioned model artifacts
- âœ… `make backtest` runs 1000-race validation
- âœ… Backtest report shows positive ROI and good calibration

---

### **PHASE 4: Qualitative Integration** (Days 29-35)
**Goal**: Fuse Deep Research insights with quantitative predictions

#### 4.1 Deep Research Pipeline
**Automated**:
- Pre-fetch overnight for next day's cards
- Structured claim extraction (JSON schema)
- Source quality scoring
- Timestamp and recency decay

**Manual Tasks**: ğŸš¨ **USER ACTION REQUIRED**
- [ ] **Set up ChatGPT Plus account** (if not already)
- [ ] **Configure Deep Research agent** (or use API if available)
- [ ] **Manual deep research workflow** (short-term):
  1. Export race card for next day
  2. Run deep research queries manually
  3. Paste results into `data/qual_claims/YYYY-MM-DD.json`
  4. System will auto-process

#### 4.2 Claim Ontology
**Automated**:
- Coarse categories (5 types):
  1. Health (positive/neutral/negative)
  2. Fitness (peak/building/underdone)
  3. Tactics (rail/mid/back preference)
  4. Gear (first-time significant)
  5. Bias (inside/outside advantage)

#### 4.3 Likelihood Ratio Learning
**Automated**:
- Start with domain expert priors
- Hierarchical Bayesian updates from outcomes
- Heavy regularization toward priors
- Shadow mode logging for validation

#### 4.4 Fusion Logic
**Automated**:
- Apply LRs post-calibration (Bayesian update)
- Re-normalize probabilities
- Track adjustments for audit trail

**Manual Tasks**: ğŸš¨ **USER ACTION REQUIRED**
- [ ] **Set LR priors** (will provide template with suggested values)
- [ ] **Review fusion outputs** - sanity check first 50 races
- [ ] **Tune LR strength** - dial up/down based on validation

**Deliverables**:
- âœ… Qualitative claims ingested and parsed
- âœ… LR fusion improves backtest Brier by â‰¥1%
- âœ… Audit trail shows rationale for each adjustment

---

### **PHASE 5: Execution & API** (Days 36-42)
**Goal**: Deploy prediction system with API and dashboard

#### 5.1 Execution Simulator
**Automated**:
- Order book dynamics model
- Partial fill simulation
- Commission and slippage
- Kelly criterion sizing (fractional)
- Market impact estimation

#### 5.2 FastAPI Backend
**Automated**:
- `/predict` endpoint (race_id â†’ probabilities)
- `/qualify` endpoint (trigger qual scan)
- `/backtest` endpoint (historical performance)
- `/meeting` endpoint (full card analysis)

#### 5.3 Streamlit Dashboard
**Automated**:
- Race card selector
- Probability table (win/place, fair odds)
- Edge calculator (vs Betfair/TAB)
- Recommended stakes (Kelly sizing)
- Uncertainty bands (conformal sets)
- Qualitative rationale (citations)
- Backtest performance charts

#### 5.4 Monitoring & Alerting
**Automated**:
- Model drift detection (PSI on features)
- Performance tracking (live Brier vs backtest)
- Data quality alerts
- API health checks

**Manual Tasks**: ğŸš¨ **USER ACTION REQUIRED**
- [ ] **Review dashboard UX** - request changes
- [ ] **Test full workflow** - ingest â†’ features â†’ predict â†’ display

**Deliverables**:
- âœ… `make serve` launches dashboard at localhost:8501
- âœ… Full race card analysis in <10 seconds
- âœ… All endpoints documented with examples

---

### **PHASE 6: Hardening & Production** (Days 43+)
**Goal**: Prepare for live operation

#### 6.1 Live Data Pipeline
**Automated**:
- Scheduled daily ingestion (cron/Prefect)
- Real-time Betfair streaming (final 5 min)
- Automatic retrain weekly
- Model A/B testing framework

#### 6.2 Paper Trading
**Manual Tasks**: ğŸš¨ **USER ACTION REQUIRED**
- [ ] **Run paper trading for 50 races** - log predictions vs outcomes
- [ ] **Validate performance** - compare to backtest expectations
- [ ] **Approve for live use** - only after 50-race validation

#### 6.3 Live Trading (Optional)
**Manual Tasks**: ğŸš¨ **USER ACTION REQUIRED**
- [ ] **Set risk limits** (max stake, daily loss limit, drawdown stop)
- [ ] **Implement kill switch** - manual override to stop all betting
- [ ] **Start with minimum stakes** - 1/10th target size for first 100 bets

---

## ğŸ”§ Technology Stack

### Core
- **Language**: Python 3.11
- **Database**: DuckDB (embedded analytical DB)
- **Data Processing**: Polars (fast dataframes)
- **ML**: CatBoost, LightGBM, scikit-learn
- **Calibration**: scikit-learn, MAPIE (conformal)
- **API**: FastAPI
- **UI**: Streamlit
- **Testing**: pytest
- **Task Runner**: Make

### Data Sources
- **Racing Data**: Racing.com, Racing Victoria (scraping)
- **Market Data**: Betfair API (Historic + Stream)
- **Weather**: Open-Meteo / BOM
- **Qualitative**: ChatGPT Deep Research (manual initially)

### DevOps
- **Environment**: VS Code Devcontainer
- **Orchestration**: Prefect (later phases)
- **Versioning**: Git + DVC (data version control)
- **Monitoring**: Prometheus + Grafana (optional)

---

## ğŸ“Š Success Metrics

### Model Performance (Backtests)
- **Brier Score**: <0.20 (market baseline ~0.18-0.19)
- **AUC**: >0.75 (rank discrimination)
- **Calibration Error**: <3% at 90% confidence band
- **ROI**: >5% after commission/slippage (1000-race test set)

### System Quality
- **Data Completeness**: >80% on key features (sectionals, weather)
- **API Latency**: <2s for `/predict` (excluding Deep Research)
- **Uptime**: >99% during racing hours
- **Test Coverage**: >80% code coverage

### Operational
- **Backtest-Live Gap**: <10% difference in Brier score
- **Max Drawdown**: <20% of bankroll (paper trading)
- **Sharpe Ratio**: >1.5 (if live trading)

---

## ğŸš¨ Manual Task Summary

### **Phase 1: Data Layer**
- [ ] Sign up for Betfair API (30 min)
- [ ] Sign up for weather API (10 min)
- [ ] Download sample Betfair historic data (1 hour)
- [ ] Test scrapers and validate outputs (1 hour)

### **Phase 2: Feature Engineering**
- [ ] Review feature distributions (30 min)
- [ ] Approve feature set (15 min)

### **Phase 3: Modeling**
- [ ] Review backtest results (1 hour)
- [ ] Approve model for next phase (15 min)

### **Phase 4: Qualitative**
- [ ] Set up ChatGPT Plus / Deep Research access (10 min)
- [ ] Run manual deep research (daily: 30 min)
- [ ] Set LR priors (30 min, one-time)
- [ ] Review fusion outputs (30 min)

### **Phase 5: API & UI**
- [ ] Review dashboard UX (30 min)
- [ ] Test full workflow (1 hour)

### **Phase 6: Production**
- [ ] Paper trading validation (50 races, ~1 week)
- [ ] Set risk limits (30 min)
- [ ] Approve for live use (decision point)

**Total Manual Time Estimate**: ~12 hours over 6 weeks

---

## ğŸ“ Repository Structure

```
racing-analysis/
â”œâ”€â”€ .devcontainer/
â”‚   â””â”€â”€ devcontainer.json         # VS Code container config
â”œâ”€â”€ .github/
â”‚   â””â”€â”€ workflows/                # CI/CD (later)
â”œâ”€â”€ configs/
â”‚   â”œâ”€â”€ database.yml              # DB schema definitions
â”‚   â”œâ”€â”€ features.yml              # Feature catalog
â”‚   â”œâ”€â”€ models.yml                # Model hyperparameters
â”‚   â””â”€â”€ scrapers.yml              # Scraper configs
â”œâ”€â”€ data/                         # Gitignored
â”‚   â”œâ”€â”€ raw/
â”‚   â”œâ”€â”€ processed/
â”‚   â”œâ”€â”€ features/
â”‚   â”œâ”€â”€ models/
â”‚   â””â”€â”€ betfair_historic/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ connectors/
â”‚   â”‚   â”œâ”€â”€ betfair.py           # Betfair API client
â”‚   â”‚   â”œâ”€â”€ racing_scraper.py   # Racing.com scraper
â”‚   â”‚   â”œâ”€â”€ stewards_scraper.py # Stewards reports
â”‚   â”‚   â””â”€â”€ weather.py           # Weather API
â”‚   â”œâ”€â”€ etl/
â”‚   â”‚   â”œâ”€â”€ ingest.py            # Data ingestion
â”‚   â”‚   â”œâ”€â”€ validate.py          # Data quality checks
â”‚   â”‚   â””â”€â”€ warehouse.py         # DuckDB interface
â”‚   â”œâ”€â”€ features/
â”‚   â”‚   â”œâ”€â”€ sectionals.py        # Pace/sectional features
â”‚   â”‚   â”œâ”€â”€ ratings.py           # Distance/going curves
â”‚   â”‚   â”œâ”€â”€ trainers.py          # Trainer/jockey metrics
â”‚   â”‚   â”œâ”€â”€ market.py            # Market features
â”‚   â”‚   â””â”€â”€ store.py             # Feature store
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ train.py             # Model training
â”‚   â”‚   â”œâ”€â”€ calibrate.py         # Calibration
â”‚   â”‚   â”œâ”€â”€ conformal.py         # Uncertainty quantification
â”‚   â”‚   â””â”€â”€ registry.py          # Model versioning
â”‚   â”œâ”€â”€ fusion/
â”‚   â”‚   â”œâ”€â”€ claims.py            # Claim extraction
â”‚   â”‚   â”œâ”€â”€ likelihood_ratios.py # LR learning
â”‚   â”‚   â””â”€â”€ fuser.py             # Bayesian fusion
â”‚   â”œâ”€â”€ execution/
â”‚   â”‚   â”œâ”€â”€ simulator.py         # Execution sim
â”‚   â”‚   â””â”€â”€ kelly.py             # Position sizing
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â”œâ”€â”€ app.py               # FastAPI application
â”‚   â”‚   â”œâ”€â”€ schemas.py           # Pydantic models
â”‚   â”‚   â””â”€â”€ routes/
â”‚   â”œâ”€â”€ dashboard/
â”‚   â”‚   â”œâ”€â”€ app.py               # Streamlit app
â”‚   â”‚   â””â”€â”€ components/
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ config.py            # Config management
â”‚       â”œâ”€â”€ logging.py           # Logging setup
â”‚       â””â”€â”€ metrics.py           # Performance metrics
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ unit/
â”‚   â”œâ”€â”€ integration/
â”‚   â””â”€â”€ fixtures/
â”œâ”€â”€ notebooks/                    # Jupyter notebooks for analysis
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ setup_db.py              # Initialize database
â”‚   â”œâ”€â”€ download_data.py         # Data download helpers
â”‚   â””â”€â”€ backtest_runner.py       # Backtest orchestration
â”œâ”€â”€ .env.example                 # Template for secrets
â”œâ”€â”€ .gitignore
â”œâ”€â”€ Makefile                     # Task automation
â”œâ”€â”€ pyproject.toml               # Python dependencies (uv/pip)
â”œâ”€â”€ README.md                    # Project overview
â””â”€â”€ MASTER_PLAN.md              # This document
```

---

## ğŸ”„ Development Workflow

### Daily Development
```bash
# Start devcontainer in VS Code
# Open terminal in container

# Ingest latest data
make ingest date=today

# Compute features
make features date=today

# Train models (if needed)
make train

# Run predictions
make predict race_id=FLE-2025-11-09-R6

# Launch dashboard
make serve
```

### Weekly Workflow
```bash
# Retrain models
make train

# Run full backtest
make backtest period=last_30_days

# Check drift
make monitor_drift

# Review performance
make report
```

### Pre-Live Checklist
```bash
# Run all tests
make test

# Validate data quality
make validate_data

# Check calibration
make check_calibration

# Simulate execution
make simulate_execution

# Review logs
make review_logs
```

---

## âš ï¸ Risk Management

### Model Risks
- **Overfitting**: Walk-forward validation, regularization, ensemble diversity
- **Concept Drift**: Weekly retraining, drift monitoring, automatic rollback
- **Data Leakage**: Strict time-ordering, manual audits of feature creation

### Execution Risks
- **Slippage**: Conservative assumptions (50% haircut on backtest ROI)
- **Market Impact**: Position size limits, liquidity checks
- **Latency**: Betfair Stream in final 5 min, <500ms order placement

### Operational Risks
- **Data Outages**: Graceful degradation, manual data entry fallback
- **API Rate Limits**: Request throttling, caching, retry logic
- **Model Failures**: Health checks, automatic fallback to simpler model

### Financial Risks
- **Drawdowns**: Kelly criterion with 1/4 to 1/2 fractional sizing
- **Stop Losses**: Maximum daily loss limit, maximum drawdown kill switch
- **Stake Limits**: Never exceed 5% of bankroll on single race

---

## ğŸ“ˆ Future Enhancements (Post-MVP)

### Advanced Modeling
- Hierarchical Bayesian logistic regression
- Full Monte Carlo pace simulation
- Meeting-level bias detection (live)
- Neural networks for non-linear interactions

### Data Expansion
- International form integration (Japan, Europe)
- Barrier trial data
- Track work data (where available)
- Jockey booking timing signals

### Qualitative Improvements
- Automated Deep Research via API (when available)
- LLM-based stewards report parsing
- Parade ring video analysis (ML vision)
- Social media sentiment (trainer/jockey confidence)

### Operations
- Multi-market support (Hong Kong, UK)
- Automated live trading
- Portfolio optimization across multiple races
- Mobile app for live updates

---

## ğŸ“ Learning Resources

### Racing Domain
- [TimeformUS Speed Figures](https://www.timeform.com/)
- [Betfair Trading Guide](https://betting.betfair.com/how-to-use-betfair/)
- [Racing Victoria Stewards](https://www.racingvictoria.com.au/the-sport/stewards)

### Modeling
- [Applied Predictive Modeling (Kuhn)](https://link.springer.com/book/10.1007/978-1-4614-6849-3)
- [Conformal Prediction Tutorial](https://github.com/valeman/awesome-conformal-prediction)
- [Calibration in Modern ML (Guo et al.)](https://arxiv.org/abs/1706.04599)

### Trading
- [Kelly Criterion](https://en.wikipedia.org/wiki/Kelly_criterion)
- [Execution Algorithms](https://www.quantstart.com/articles/)

---

## âœ… Current Status

**Phase**: 0 - Foundation  
**Progress**: 10% (plan created, ready to build)  
**Next Steps**: 
1. Create repository structure
2. Set up devcontainer
3. Initialize database schema
4. Begin Phase 1 data connectors

**Last Updated**: November 8, 2025  
**Owner**: thomasrocks006-cmyk

---

## ğŸ“ Change Log

| Date | Phase | Change | Reason |
|------|-------|--------|--------|
| 2025-11-08 | Planning | Initial plan created | Project kickoff |
| 2025-11-08 | Planning | Removed licensing requirements | Non-commercial use |
| 2025-11-08 | Planning | Adjusted Deep Research cost model | ChatGPT Plus available |

---

**Ready to build. Let's start with Phase 0.**
